import pandas as pd
import numpy as np
import re
from tqdm import tqdm

# Liste des colonnes n√©cessaires pour le matching
COLONNES_REQUISES = [
    'nom', 'prenoms', 'naissance_date', 
    'cin', 'cin_lieu', 'telephone', 'mail', 
    'id_mention', # Conserv√© par pr√©caution
    'composante', # Ajout√© pour la nouvelle cl√©
    'mention' # Ajout√© pour la nouvelle cl√©
]

# Cl√©s de concat√©nation qui serviront de base √† la d√©tection de doublons
# La cl√© 'np_id_mention' est renomm√©e en 'np_composante_mention' pour refl√©ter la nouvelle logique.
KEY_COLUMNS = [
    'np_naissance', 
    'np_cin', 
    'np_cin_lieu', 
    'np_telephone',
    'np_mail',
    'np_composante_mention' # Cl√© faible, utilisant composante et mention
]

# Cl√©s d'identit√© fortes pour le contr√¥le conditionnel (non utilis√©es dans le cha√Ænage actuel)
STRONG_IDENTITY_COLUMNS = ['cin', 'naissance_date']


# --- Fonctions de Nettoyage et de Pr√©paration ---

def standardiser_champs_pour_hachage(df: pd.DataFrame) -> pd.DataFrame:
    """
    Standardise les champs Nom et Pr√©noms pour une meilleure robustesse des cl√©s.
    Supprime les caract√®res non alphanum√©riques et met en majuscule.
    
    Cr√©e 'nom_prenoms_standard' qui tol√®re les pr√©noms nuls, tant que le nom est pr√©sent.
    """
    print("--- ‚öôÔ∏è Standardisation des champs de base pour le Hachage ---")

    # Colonnes √† nettoyer sp√©cifiquement pour la cr√©ation de cl√©s
    # Ajout de composante et mention au nettoyage standard
    cols_a_nettoyer = ['nom', 'prenoms', 'cin_lieu', 'mail', 'composante', 'mention'] 
    
    # Pr√©traitement de toutes les colonnes requises
    for col in tqdm(cols_a_nettoyer, desc="Nettoyage des colonnes pour cl√©s"):
        if col in df.columns:
            # Remplacer NaN par cha√Æne vide, mettre en majuscule, supprimer les espaces multiples et caract√®res sp√©ciaux
            df[f'{col}_standard'] = df[col].astype(str).str.upper().str.strip()
            # Suppression des caract√®res non alphanum√©riques pour un matching strict (sauf CIN d√©j√† format√©)
            if col not in ['cin', 'telephone']: # Ces colonnes devraient d√©j√† √™tre propres
                df[f'{col}_standard'] = df[f'{col}_standard'].str.replace(r'[^A-Z0-9]', '', regex=True)
            
            df[f'{col}_standard'] = df[f'{col}_standard'].replace('', pd.NA)
        else:
            # Cr√©er la colonne standard si elle n'existe pas, la remplir avec NA
            df[f'{col}_standard'] = pd.NA
            tqdm.write(f"‚ö†Ô∏è Colonne '{col}' manquante, remplie avec NA.")
    
    # Cr√©ation du champ Nom_Prenoms standard (Autorise Pr√©noms NULL)
    df['nom_prenoms_standard'] = pd.NA
    
    # R√©cup√©ration des versions standardis√©es, avec gestion si la colonne n'a pas pu √™tre cr√©√©e
    nom_std = df.get('nom_standard', pd.Series(pd.NA, index=df.index))
    prenoms_std = df.get('prenoms_standard', pd.Series(pd.NA, index=df.index))
    
    # La concat√©nation est possible si 'nom' est non nul.
    condition_nom_ok = nom_std.notna() 
    
    df.loc[condition_nom_ok, 'nom_prenoms_standard'] = \
        nom_std.fillna('') + prenoms_std.fillna('')
        
    df['nom_prenoms_standard'] = df['nom_prenoms_standard'].replace('', pd.NA) # Ne doit pas √™tre une cha√Æne vide
    
    # Assurer que 'id_mention' est pr√©sent (pour la nouvelle cl√©)
    if 'id_mention' not in df.columns:
        df['id_mention'] = pd.NA
        tqdm.write(f"‚ö†Ô∏è Colonne 'id_mention' manquante, remplie avec NA.")
        
    return df

def creer_cles_de_concatenation(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cr√©e les colonnes de concat√©nation robustes demand√©es, y compris la cl√© np_composante_mention.
    """
    print("\n--- üóùÔ∏è Cr√©ation des Cl√©s de Concatenation ---")
    
    # Configuration des cl√©s
    cles_config = {
        'np_naissance': ['nom_prenoms_standard', 'naissance_date'],
        'np_cin': ['nom_prenoms_standard', 'cin'],
        'np_cin_lieu': ['nom_prenoms_standard', 'cin_lieu'],
        'np_telephone': ['nom_prenoms_standard', 'telephone'],
        'np_mail': ['nom_prenoms_standard', 'mail'],
        # Nouvelle cl√© bas√©e sur Nom/Pr√©noms, Composante et Mention
        'np_composante_mention': ['nom_prenoms_standard', 'composante_standard', 'mention_standard'],
    }

    # S'assurer que les colonnes de r√©f√©rence existent (y compris les versions standardis√©es)
    reference_cols = [
        'nom_prenoms_standard', 'naissance_date', 'cin', 'cin_lieu', 
        'telephone', 'mail', 'id_mention', 'composante_standard', 'mention_standard'
    ]
    
    for col in reference_cols:
        if col not in df.columns:
            df[col] = pd.Series(pd.NA, index=df.index)
    
    for key_name, components in tqdm(cles_config.items(), desc="G√©n√©ration des cl√©s"):
        
        # Les composants sont les colonnes √† utiliser pour le hachage
        
        # 1. Condition de non-nullit√© : TOUS les composants DOIVENT √™tre non nuls
        condition_creation = df['nom_prenoms_standard'].notna()
        # Assurer que tous les composants additionnels sont non nuls
        for comp in components[1:]: 
            condition_creation &= df[comp].notna()

        df[key_name] = pd.NA # Initialisation de la colonne √† NA
        
        if condition_creation.any():
            # Pr√©traitement des composants pour la concat√©nation (uniquement pour les lignes √©ligibles)
            cols_to_concat = []
            for col in components:
                
                # Utilisation des donn√©es pour les lignes concern√©es
                data_series = df.loc[condition_creation, col].astype(str).str.upper()
                
                # NETTOYAGE R√âTROACTIF POUR LES CL√âS CRITIQUES 
                if col in ['naissance_date', 'cin', 'telephone']:
                    # Nettoyage sp√©cifique pour les ID bruts
                    cleaned_data = data_series.str.replace(r'[^A-Z0-9-]', '', regex=True)
                else:
                    # nom_prenoms_standard et les autres sont d√©j√† nettoy√©s et en majuscules
                    cleaned_data = data_series
                    
                cols_to_concat.append(cleaned_data)
            
            # Concat√©nation avec '_'
            new_keys = cols_to_concat[0]
            for i in range(1, len(cols_to_concat)):
                 # Concat√©nation de tous les composants avec '_'
                 new_keys = new_keys.str.cat(cols_to_concat[i], sep='_')

            # Application des nouvelles cl√©s sur les lignes √©ligibles
            df.loc[condition_creation, key_name] = new_keys
            
        else:
            tqdm.write(f"‚ÑπÔ∏è Cl√© {key_name} non g√©n√©r√©e : aucun enregistrement valide.")
            
    print("‚úÖ Cl√©s de concat√©nation cr√©√©es avec la r√®gle de non-nullit√© stricte.")
    return df

# --- NOUVELLE FONCTION : V√©rification de Contradiction Forte ---

def verifier_contradiction_forte(df: pd.DataFrame, indices_a_tester: pd.Index, colonnes_fortes: list) -> bool:
    """
    V√©rifie si un ensemble d'enregistrements (indices) pr√©sente une forte contradiction 
    sur les identifiants cl√©s (CIN, Date de Naissance).
    
    Une forte contradiction est d√©tect√©e si, pour l'une des colonnes_fortes, il existe 
    plus d'une valeur non-NA unique.
    
    :param df: Le DataFrame principal contenant les donn√©es nettoy√©es.
    :param indices_a_tester: Les indices des lignes √† v√©rifier.
    :param colonnes_fortes: Liste des colonnes fortes √† v√©rifier (ex: ['cin', 'naissance_date']).
    :return: True si une contradiction est trouv√©e, False sinon.
    """
    # Le test doit porter sur les valeurs consolid√©es/nettoy√©es de df
    df_test = df.loc[indices_a_tester]
    
    for col in colonnes_fortes:
        # R√©cup√©rer les valeurs uniques non-NA
        # df[col] est cens√© √™tre d√©j√† nettoy√© (cin format√©, naissance_date en datetime)
        unique_values = df_test[col].dropna().unique()
        
        # Si plus d'une valeur unique non-NA est trouv√©e, il y a contradiction
        if len(unique_values) > 1:
            return True
            
    return False

# --- Fonction Principale d'Assignation de Code ---

def gerer_code_etudiant_et_consolider(df: pd.DataFrame, hash_algorithm: str) -> pd.DataFrame:
    """
    Attribue un code √©tudiant unique, regroupant les doublons identifi√©s
    par les cl√©s de concat√©nation, puis consolide les champs.
    
    L'√©tape 4 impl√©mente l'algorithme de cha√Ænage avec convergence garantie
    pour toutes les cl√©s (y compris np_composante_mention), en appliquant
    une condition d'exclusion pour la cl√© faible.
    """
    if df.empty:
        print("DataFrame vide. Aucun code √©tudiant assign√©.")
        return df

    print("\n--- üî¢ Attribution des Codes √âtudiants et D√©tection de Doublons ---")

    # √âtape 1 : Initialisation de l'ID temporaire unique pour chaque ligne
    df['id_temporaire'] = df.index + 1
    
    # √âtape 2 : Standardisation des champs pour la cr√©ation des cl√©s
    df = standardiser_champs_pour_hachage(df)
    
    # √âtape 3 : Cr√©ation des cl√©s de matching
    df = creer_cles_de_concatenation(df)

    # √âtape 4 : Propagation du plus petit ID pour regrouper les doublons (Algorithme de cha√Ænage)
    
    # Utiliser une copie des colonnes pertinentes pour les manipulations d'ID
    df_temp = df[['id_temporaire'] + KEY_COLUMNS].copy()
    
    # Colonnes fortes √† v√©rifier pour la r√®gle conditionnelle (elles doivent √™tre pr√©sentes et nettoy√©es dans df)
    COLONNES_FORTES_CHECK = ['cin', 'naissance_date'] 

    # S'assurer que les colonnes cl√©s sont de type StringDtype pour la robustesse
    for col in KEY_COLUMNS:
        df_temp[col] = df_temp[col].astype(pd.StringDtype())

    total_doublons = 0
    iteration = 0
    
    # Algorithme de cha√Ænage : it√©rer jusqu'√† convergence (nouvelles_fusions == 0)
    while True:
        iteration += 1
        nouvelles_fusions = 0
        tqdm.write(f"\n--- It√©ration {iteration} : D√©tection et Cha√Ænage ---")
        
        # Copie des ID actuels pour que les calculs de 'min' soient coh√©rents
        id_temp_current = df_temp['id_temporaire'].copy()
        
        for key_col in tqdm(KEY_COLUMNS, desc=f"Regroupement par cl√©"):
            
            mask_not_na = df_temp[key_col].notna()
            df_subset = df_temp.loc[mask_not_na].copy() # Copie pour le groupement
            
            if df_subset.empty:
                continue

            # 1. Calcul de l'ID Canonique (le plus petit ID du groupe)
            grouped = df_subset.groupby(key_col)
            canonical_ids = grouped['id_temporaire'].transform('min')
            
            # 2. Condition de fusion: L'ID actuel doit √™tre plus grand que l'ID canonique
            condition_fusion_finale = (df_subset['id_temporaire'] > canonical_ids)
            
            # Indices des lignes qui proposent une mise √† jour d'ID dans cette cl√©
            indices_a_maj = condition_fusion_finale[condition_fusion_finale].index
            
            if len(indices_a_maj) > 0:
                
                indices_valides = indices_a_maj # Par d√©faut, toutes les fusions sont valides
                
                # --- R√àGLE CONDITIONNELLE D'EXCLUSION pour la cl√© faible ---
                if key_col == 'np_composante_mention':
                    
                    # Mapping des fusions: ID actuel -> ID Canonique propos√©
                    propositions = pd.DataFrame({
                        'id_courant': df_temp.loc[indices_a_maj, 'id_temporaire'],
                        'id_cible': canonical_ids.loc[indices_a_maj]
                    }).reset_index() # Conserve l'index original

                    # Identifier les fusions qui joignent deux groupes diff√©rents (id_courant != id_cible)
                    fusions_inter_groupes = propositions[propositions['id_courant'] != propositions['id_cible']].copy()

                    # On ne teste qu'une seule fois la fusion d'un groupe A vers B
                    groupes_a_tester = fusions_inter_groupes.drop_duplicates(subset=['id_courant', 'id_cible'])
                    
                    # ID des cibles (id_cible) qui ont √©t√© test√©es et valid√©es
                    groupes_valides_cible_id = set()
                    
                    for _, row in groupes_a_tester.iterrows():
                        id_courant = row['id_courant']
                        id_cible = row['id_cible']
                        
                        # R√©cup√©rer tous les indices des lignes faisant partie des deux groupes (avant fusion)
                        indices_courant = df_temp[df_temp['id_temporaire'] == id_courant].index
                        indices_cible = df_temp[df_temp['id_temporaire'] == id_cible].index
                        indices_combines = indices_courant.union(indices_cible)

                        # V√©rification de Contradiction Forte
                        if not verifier_contradiction_forte(df, indices_combines, COLONNES_FORTES_CHECK):
                            # Si AUCUNE contradiction, autoriser la fusion
                            groupes_valides_cible_id.add(id_cible)
                            
                    
                    # Le filtre: seules les lignes dont l'id_cible a √©t√© valid√© sont conserv√©es
                    # On conserve aussi les fusions intra-groupe (id_courant == id_cible)
                    indices_valides = propositions[
                        (propositions['id_cible'].isin(groupes_valides_cible_id)) | 
                        (propositions['id_courant'] == propositions['id_cible'])    
                    ]['index']

                # --- Fin de la R√®gle Conditionnelle ---
                
                # R√©cup√©rer les nouvelles valeurs canoniques pour les indices valid√©s/non-filtr√©s
                canonical_values_validated = canonical_ids.loc[indices_valides]
                
                # Appliquer la mise √† jour dans df_temp (utilise les indices originaux)
                df_temp.loc[indices_valides, 'id_temporaire'] = canonical_values_validated
                nouvelles_fusions += len(indices_valides)
                
            # Mise √† jour du total des doublons (sera utilis√© dans le message de conclusion)
            if key_col == 'np_composante_mention' and iteration == 1:
                 # Seules les fusions valid√©es sont compt√©es
                 tqdm.write(f"   (Cl√© faible) Fusions valid√©es par np_composante_mention : {len(indices_valides)}")
            
        total_doublons += nouvelles_fusions
        
        # Test de convergence
        if nouvelles_fusions == 0:
            tqdm.write("Pas de nouvelles fusions d√©tect√©es. Le processus a converg√©.")
            break
        elif iteration == 1:
            tqdm.write(f"Fusion de {nouvelles_fusions} liens d√©tect√©e. Continuer l'it√©ration pour cha√Ænage.")
            
    # √âtape 5 : Mise √† jour du DataFrame original
    df['id_groupe'] = df_temp['id_temporaire']
    
    # √âtape 6 : Finalisation du code √©tudiant
    # Attribution d'un num√©ro s√©quentiel unique √† chaque groupe d'√©tudiants (code_final_sequence)
    codes_uniques = df.groupby('id_groupe').ngroup() + 1
    df['code_final_sequence'] = codes_uniques

    df['code_etudiant'] = 'ETU_' + df['code_final_sequence'].astype(str).str.zfill(8)

    # --- √âtape 7 : Consolidation des champs (Imputation des valeurs non nulles) ---
    colonnes_consolidation = [
        'nom', 'prenoms', 'cin', 'cin_date', 'cin_lieu', 'nationalite', 'naissance_lieu', 
        'mail', 'telephone', 'adresse', 'sexe', 'bacc_annee', 'bacc_serie_technique', 'bacc_serie', 
        'bacc_numero', 'bacc_centre', 'bacc_mention',
        'naissance_date', 'naissance_annee', 'naissance_mois', 'naissance_jour', 
        'id_mention', 'composante', 'mention' # Ajout pour consolidation
    ]

    print("\n--- √âTAPE 7 : CONSOLIDATION DES CHAMPS (IMPUTATION) ---")
    
    for col in tqdm(colonnes_consolidation, desc="Consolidation des valeurs non-NA"):
        if col in df.columns:
             # Convertir en string si c'est de type objet pour le remplacement
            is_object = df[col].dtype == 'object' or df[col].dtype.name == 'string'
            if is_object:
                # Remplacer les cha√Ænes vides par NaN pour garantir que transform('first') fonctionne correctement
                df[col] = df[col].replace('', np.nan) 
            
            # Application de la consolidation: propager la premi√®re valeur non-NA
            # Groupement par l'ID de groupe final
            df[col] = df.groupby('id_groupe', dropna=False)[col].transform('first')
            
            # Tenter de remettre le type de donn√©es initial si possible
            if is_object:
                df[col] = df[col].convert_dtypes() # Utiliser StringDtype si possible

    print("‚úÖ Consolidation des champs (imputation des valeurs non nulles des doublons) effectu√©e.")


    # --- √âtape 8 : Nettoyage final ---
    # Suppression des colonnes de travail
    cols_a_dropper = [col for col in df.columns if col.endswith('_standard') or col in KEY_COLUMNS or col.startswith('id_groupe') or col.startswith('code_final_sequence') or col.startswith('id_temporaire')]
    df = df.drop(columns=cols_a_dropper, errors='ignore')

    nombre_codes_uniques = df['code_etudiant'].nunique()
    lignes_total = len(df)
    
    print(f"\n‚úÖ Traitement termin√© : {lignes_total} lignes, {nombre_codes_uniques} codes √©tudiants uniques.")
    print(f"   {total_doublons} lignes ont √©t√© regroup√©es gr√¢ce aux cl√©s de doublons.")
    
    return df
